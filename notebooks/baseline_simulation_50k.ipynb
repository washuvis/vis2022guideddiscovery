{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('../data/simulation_study/')\n",
    "from knn_models import KNNModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_load_npz(file):\n",
    "    \"\"\"\n",
    "    allows pickle files as opposed to the scipy version\n",
    "    \"\"\"\n",
    "    with np.load(file, allow_pickle=True) as loaded:\n",
    "        try:\n",
    "            matrix_format = loaded['format']\n",
    "        except KeyError as e:\n",
    "            raise ValueError('The file {} does not contain a sparse matrix.'.format(file)) from e\n",
    "\n",
    "        matrix_format = matrix_format.item()\n",
    "\n",
    "        if not isinstance(matrix_format, str):\n",
    "            # Play safe with Python 2 vs 3 backward compatibility;\n",
    "            # files saved with SciPy < 1.0.0 may contain unicode or bytes.\n",
    "            matrix_format = matrix_format.decode('ascii')\n",
    "\n",
    "        try:\n",
    "            cls = getattr(scipy.sparse, '{}_matrix'.format(matrix_format))\n",
    "        except AttributeError as e:\n",
    "            raise ValueError('Unknown matrix format \"{}\"'.format(matrix_format)) from e\n",
    "\n",
    "        if matrix_format in ('csc', 'csr', 'bsr'):\n",
    "            return cls((loaded['data'], loaded['indices'], loaded['indptr']), shape=loaded['shape'])\n",
    "        elif matrix_format == 'dia':\n",
    "            return cls((loaded['data'], loaded['offsets']), shape=loaded['shape'])\n",
    "        elif matrix_format == 'coo':\n",
    "            return cls((loaded['data'], (loaded['row'], loaded['col'])), shape=loaded['shape'])\n",
    "        else:\n",
    "            raise NotImplementedError('Load is not implemented for '\n",
    "                                      'sparse matrix of format {}.'.format(matrix_format))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAST Challenge Dataset\n",
    "\n",
    "# load the full VAST Challenge 2011 microblog dataset\n",
    "all_data_microblogs = pd.read_pickle(f'../data/user_study/pickle/microblogs_dataset.pkl.gz', compression='gzip')\n",
    "\n",
    "# list of keyword stems indicating a tweet is relevant\n",
    "keywords = ['sore', 'throat', 'fever', 'fatigu', 'cough', 'short', 'breath', 'chill', 'sick', \n",
    "            'pain', 'diarrhea', 'stomach', 'sweat', 'pneumonia', 'flu', 'ach', 'nausea', 'vomit', \n",
    "            'nauseou', 'declin', 'health', 'headach', 'nose', 'runni']\n",
    "\n",
    "# Label data points as relevant (1) or irrelevant (0) according to the keywords above\n",
    "all_data_microblogs['label'] =  all_data_microblogs.apply(lambda row: int(any([w in keywords for w in row.porter_stems])), axis=1)\n",
    "\n",
    "subset_data_ids_2 = np.load(f'../data/simulation_study/model_50000_1/ids.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 50\n",
    "total_budget = 250\n",
    "\n",
    "knn_text_weights = my_load_npz('../data/simulation_study/model_50000_1/text_cos_weights_weighted.npz')\n",
    "knn_text_model = KNNModel([0.99, 0.01], knn_text_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_experiments(policy, num_experiments=num_experiments, total_budget=total_budget):\n",
    "    num_discoveries = np.array([])\n",
    "    num_keywords = np.array([])\n",
    "    model_recall = np.array([])\n",
    "    model_f1_score = np.array([])\n",
    "    model_auc_score = np.array([])\n",
    "\n",
    "    for exp in range(num_experiments):\n",
    "\n",
    "        train_ind = np.random.choice([i for i in range(len(subset_data_ids_2)) if all_data_microblogs.iloc[subset_data_ids_2[i]]['label'] == 1], 1)\n",
    "        train_labels = np.array([1 for i in train_ind])\n",
    "        all_indices = np.array(range(len(subset_data_ids_2)))\n",
    "\n",
    "        for iteration in range(total_budget):\n",
    "            test_ind = np.delete(all_indices, np.argwhere(np.isin(all_indices, train_ind)))\n",
    "            \n",
    "            if policy == 'random':\n",
    "                queries = np.random.choice(test_ind, 1)\n",
    "                train_ind = np.append(train_ind, queries[0])\n",
    "                l = all_data_microblogs.iloc[subset_data_ids_2[queries[0]]]['label']\n",
    "                train_labels = np.append(train_labels, l)\n",
    "                \n",
    "            elif policy =='as_greedy':\n",
    "                probs = knn_text_model.predict(test_ind, train_ind, train_labels)\n",
    "                max_prob = np.max(probs)\n",
    "                max_inds = np.argwhere(probs==max_prob).ravel()\n",
    "                queries = np.random.choice(test_ind[max_inds], 1)\n",
    "                train_ind = np.append(train_ind, queries[0])\n",
    "                l = all_data_microblogs.iloc[subset_data_ids_2[queries[0]]]['label']\n",
    "                train_labels = np.append(train_labels, l)\n",
    "                \n",
    "                \n",
    "            elif policy == 'unc':\n",
    "                probs = knn_text_model.predict(test_ind, train_ind, train_labels)\n",
    "                unc = np.abs(probs - 0.5)\n",
    "                unc_min = np.min(unc)\n",
    "                min_inds = np.argwhere(unc==unc_min).ravel()\n",
    "                queries = np.random.choice(test_ind[min_inds], 1)\n",
    "                train_ind = np.append(train_ind, queries[0])\n",
    "                l = all_data_microblogs.iloc[subset_data_ids_2[queries[0]]]['label']\n",
    "                train_labels = np.append(train_labels, l)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                raise Exception\n",
    "\n",
    "        print('-', end='')\n",
    "\n",
    "        # Discovery\n",
    "        num_discoveries = np.append(num_discoveries, all_data_microblogs.iloc[subset_data_ids_2[train_ind[1:]]]['label'].sum())\n",
    "\n",
    "        # Detection\n",
    "        kw_detected = np.intersect1d(np.array(keywords), np.array(all_data_microblogs.iloc[subset_data_ids_2[train_ind[1:]]].porter_stems.sum()))\n",
    "        num_keywords = np.append(num_keywords, len(kw_detected))\n",
    "\n",
    "        # Training\n",
    "        test_ind = np.delete(all_indices, np.argwhere(np.isin(all_indices, train_ind)))\n",
    "        train_labels = all_data_microblogs.iloc[subset_data_ids_2[train_ind]]['label']\n",
    "        probs = knn_text_model.predict(test_ind, train_ind, train_labels)\n",
    "        inferred_labels = (probs>=0.5).astype(int)\n",
    "        true_label = all_data_microblogs.iloc[subset_data_ids_2[test_ind]]['label']\n",
    "        accuracy = (true_label == inferred_labels).sum() / len(test_ind)\n",
    "        model_recall = np.append(model_recall, recall_score(true_label, inferred_labels))\n",
    "        model_f1_score = np.append(model_f1_score, f1_score(true_label, inferred_labels))\n",
    "        model_auc_score = np.append(model_auc_score, roc_auc_score(true_label, probs))\n",
    "\n",
    "\n",
    "    print(f'\\n{policy}')\n",
    "    print (f'Discovery: {num_discoveries.mean():.2f} ± {1.98 * num_discoveries.std()/np.sqrt(len(num_discoveries)):.3f}')\n",
    "    print (f'Detection: {num_keywords.mean():.2f} ± {1.98 * num_keywords.std()/np.sqrt(len(num_keywords)):.3f}')\n",
    "    print (f'Recall: {model_recall.mean():.2f} ± {1.98 * model_recall.std()/np.sqrt(len(model_recall)):.3f}')\n",
    "    print (f'F1 Score: {model_f1_score.mean():.2f} ± {1.98 * model_f1_score.std()/np.sqrt(len(model_f1_score)):.3f}')\n",
    "    print (f'ROC-AUC Score: {model_auc_score.mean():.2f} ± {1.98 * model_auc_score.std()/np.sqrt(len(model_auc_score)):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments('random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments('as_greedy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments('unc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
